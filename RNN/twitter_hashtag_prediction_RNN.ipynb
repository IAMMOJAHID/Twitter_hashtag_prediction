{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUEfkUcJZuWp",
        "outputId": "90b082c0-2cb6-495e-87a6-8e494f5a0707"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from statistics import mode\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import SimpleRNN,Input, LSTM, Bidirectional, Embedding,Dense,Concatenate,Attention, Average\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYVw4ACTaBgA",
        "outputId": "d2d0b3ed-5048-4953-875b-b555fd02e426"
      },
      "outputs": [],
      "source": [
        "#read the dataset file\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "#tweet column is input\n",
        "inp_data=train[\"tweet\"]\n",
        "#target data is sentiment(s1,s2,s3,s4,s5) ,\n",
        "#when (w1,w2,w3,w4) and kind(k1,k2,k3...k15)\n",
        "tar_data=train.iloc[:,4:].values\n",
        "print(\"tar data:\", tar_data)\n",
        "\n",
        "#get the column name of target\n",
        "tar_lab=train.iloc[:,4:].columns.tolist()\n",
        "print(\"tar_lab:\", tar_lab)\n",
        "#value of the target label like\n",
        "#s1=\"I can't tell\" , s2=\"Negative\" and so on till s5\n",
        "#w1=\"current weather\", w2=future forecast and so on till w4\n",
        "#k1=\"clouds\", k2=\"cold\", k3=\"dry\" and so on till k15\n",
        "tar_lab_val=[\n",
        "\"I can't tell\",\"Negative\",\"Neutral\",\"Positive\",\"Tweet not related to weather condition\",\n",
        "\"current (same day) weather\",\"future (forecast)\",\"I can't tell\",\"past weather\",\n",
        "\"clouds\",\"cold\",\"dry\",\"hot\",\"humid\",\"hurricane\",\"I can't tell\",\"ice\",\"other\",\"rain\",\n",
        "\"snow\",\"storms\",\"sun\",\"tornado\",\"wind\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_IIVR5eGaG-5"
      },
      "outputs": [],
      "source": [
        "#clean the tweets\n",
        "def clean(tweet):\n",
        "  #replace and lower the tweets\n",
        "  tweet=tweet.replace(\":\",\"\").lower()\n",
        "  #get only words that contains alphabets\n",
        "  words= list(filter(lambda w:(w.isalpha()),tweet.split(\" \")))\n",
        "  #expand the shortened words\n",
        "  words= [contractions[w] if w in contractions else w for w in words ]\n",
        "  #return all the words\n",
        "  return words\n",
        "\n",
        "\n",
        "inp_texts=[]\n",
        "tar_texts=[]\n",
        "inp_words=[]\n",
        "tar_words=[]\n",
        "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TskMmjqyaKZT",
        "outputId": "57019b57-bc7a-4cc6-c8e3-b01e3d4ae3c0"
      },
      "outputs": [],
      "source": [
        "#iterate over input data\n",
        "for tweet in inp_data:\n",
        "  #clean the tweets\n",
        "  inpt_words= clean(tweet)\n",
        "  #store the input texts and words\n",
        "  inp_texts+= [' '.join(inpt_words)]\n",
        "  inp_words+= inpt_words\n",
        "\n",
        "#iterate over target data\n",
        "for lab in tar_data:\n",
        "  #get index of maximum value from sentiment data(s1 to s5)\n",
        "  #with the help of this index get label value\n",
        "  senti=tar_lab[np.argmax(lab[:5])]\n",
        "  #get index of maximum value from when data(w1 to w4)\n",
        "  #with the help of this index get label value\n",
        "  when=tar_lab[np.argmax(lab[5:9])+5]\n",
        "  #get index of values greater than 0.5 and get label value from it\n",
        "  kind=[tar_lab[ind] for ind,ele in enumerate(lab[9:len(lab)],9) if ele>=0.5]\n",
        "  #store the target text which is combination of sentiment,when and kind data\n",
        "  #add sos at start and eos at end of text\n",
        "  tar_texts+=[\"sos \"+\" \".join([senti]+[when]+kind)+\" eos\"]\n",
        "\n",
        "print(\"inp_texts:\", inp_texts[1:5])\n",
        "print(\"tar_texts:\", tar_texts[1:5])\n",
        "print(\"inp_words:\", inp_words[1:5])\n",
        "print(\"tar_words:\", tar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mkNkaMzaM0b",
        "outputId": "03fa2917-69f3-4414-9bdf-bd22d7640194"
      },
      "outputs": [],
      "source": [
        "#only store unique words from the input and target word lists\n",
        "inp_words = sorted(list(set(inp_words)))\n",
        "num_inp_words = len(inp_words)\n",
        "num_tar_words = len(tar_lab)+2\n",
        "\n",
        "#get the length of the input and the target texts which appears most frequently\n",
        "max_inp_len = mode([len(i) for i in inp_texts])\n",
        "max_tar_len = mode([len(i) for i in tar_texts])\n",
        "\n",
        "print(\"number of input words : \",num_inp_words)\n",
        "print(\"number of target words : \",num_tar_words)\n",
        "print(\"maximum input length : \",max_inp_len)\n",
        "print(\"maximum target length : \",max_tar_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_JsmyKeaO_v",
        "outputId": "f54bac07-13bc-4e9a-8313-d710b45182a8"
      },
      "outputs": [],
      "source": [
        "#split the input and target text into 90:10 ratio or testing size of 10%=0.1.\n",
        "x_train,x_test,y_train,y_test=train_test_split(inp_texts,tar_texts,test_size=0.1,random_state=42)\n",
        "\n",
        "#Use all of the words from training input and output to train the tokenizer.\n",
        "inp_tokenizer = Tokenizer()\n",
        "inp_tokenizer.fit_on_texts(x_train)\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#convert text to an integer sequence where the integer represents the word index\n",
        "x_train= inp_tokenizer.texts_to_sequences(x_train)\n",
        "y_train= tar_tokenizer.texts_to_sequences(y_train)\n",
        "\n",
        "print(\"x_train:\",x_train[1:5])\n",
        "print(\"y_train:\",y_train[1:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n8C4MNLXaRFI"
      },
      "outputs": [],
      "source": [
        "#If the length is less than the maximum length, pad the array with 0s.\n",
        "enc_inp_data= pad_sequences(x_train, maxlen=max_inp_len, padding='post',dtype=\"float32\")\n",
        "dec_data= pad_sequences(y_train, maxlen=max_tar_len, padding='post',dtype=\"float32\")\n",
        "\n",
        "#The last word, ie 'eos,' will not be included in the decoder input data.\n",
        "dec_inp_data = dec_data[:,:-1]\n",
        "\n",
        "#decoder target data will be one time step ahead as it will not include the first initial word i.e 'sos'\n",
        "dec_tar_data = dec_data.reshape(len(dec_data),max_tar_len,1)[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LKZO0_vWaTan",
        "outputId": "f11337b2-564d-4173-f961-b2bc1f84d372"
      },
      "outputs": [],
      "source": [
        "# Model formation\n",
        "\n",
        "K.clear_session()\n",
        "latent_dim = 500\n",
        "\n",
        "#create input object with the shape equal to the maximum number of input words\n",
        "enc_inputs = Input(shape=(max_inp_len,))\n",
        "enc_embedding = Embedding(num_inp_words+1, latent_dim)(enc_inputs)\n",
        "\n",
        "#create 3 stacked LSTM layer\n",
        "#1st LSTM layer keep only output\n",
        "enc_lstm1= SimpleRNN(latent_dim, return_state=True, return_sequences=True)\n",
        "enc_outputs1, *_ = enc_lstm1(enc_embedding)\n",
        "\n",
        "#2nd LSTM layer keep only output\n",
        "enc_lstm2= SimpleRNN(latent_dim, return_state=True, return_sequences=True)\n",
        "enc_outputs2, *_ = enc_lstm2(enc_outputs1)\n",
        "\n",
        "#3rd LSTM layer keep output as well as its states\n",
        "enc_lstm3= SimpleRNN(latent_dim,return_sequences=True,return_state=True)\n",
        "enc_outputs3 , state_h3= enc_lstm3(enc_outputs2)\n",
        "\n",
        "#encoder states\n",
        "enc_states= [state_h3]\n",
        "\n",
        "# Decoder.\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_tar_words+1, latent_dim)\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "#initialize the LSTM layer of the decoder with the encoder's output states\n",
        "dec_lstm = SimpleRNN(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "\n",
        "#Attention layer\n",
        "attention =Attention()\n",
        "attn_out = attention([dec_outputs,enc_outputs3])\n",
        "\n",
        "#Merge the attention output with the decoder outputs\n",
        "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])\n",
        "\n",
        "#fully connected Dense layer for the output\n",
        "dec_dense = Dense(num_tar_words+1, activation='softmax')\n",
        "dec_outputs = dec_dense(merge)\n",
        "\n",
        "#Model class and model summary\n",
        "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "model.summary()\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "idVPJeSQaW2U"
      },
      "outputs": [],
      "source": [
        "#compile the model using RMSProp optimizer\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#train the model with input and target data from encoder and decoder\n",
        "# model.fit( [enc_inp_data, dec_inp_data], dec_tar_data, batch_size=500, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pi7WgF7KacF7"
      },
      "outputs": [],
      "source": [
        "#Save model with the name as “s2s”\n",
        "# model.save('my_model.keras')\n",
        "\n",
        "#encoder inference\n",
        "latent_dim=500\n",
        "\n",
        "# #load the model\n",
        "model = models.load_model(\"my_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eq_8KgjAaeC2"
      },
      "outputs": [],
      "source": [
        "# Encoder inference\n",
        "\n",
        "# Construct an encoder model for inference\n",
        "enc_outputs, state_h_enc = model.layers[6].output\n",
        "enc_states = [state_h_enc]  # SimpleRNN uses only a hidden state\n",
        "enc_model = Model(model.input[0], [enc_outputs] + enc_states)\n",
        "\n",
        "# Decoder inference model\n",
        "# Create Input object for hidden state for the decoder\n",
        "dec_state_input_h = Input(shape=(latent_dim,))\n",
        "dec_hidden_state_input = Input(shape=(max_inp_len, latent_dim))\n",
        "\n",
        "# Get the decoder layers from the trained model\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_rnn = model.layers[7]\n",
        "attention = model.layers[8]\n",
        "dec_dense = model.layers[10]\n",
        "\n",
        "# Embed the decoder inputs\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "# Pass decoder inputs and initialize the SimpleRNN layer with the decoder’s hidden state\n",
        "dec_outputs2, state_h2 = dec_rnn(dec_embedding, initial_state=[dec_state_input_h])\n",
        "\n",
        "# Apply attention mechanism\n",
        "attn_out1 = attention([dec_outputs2, dec_hidden_state_input])\n",
        "\n",
        "# Merge attention output with decoder outputs\n",
        "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out1])\n",
        "\n",
        "# Dense layer for decoder output\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# Define the decoder model class\n",
        "dec_model = Model(\n",
        "    [dec_inputs] + [dec_hidden_state_input, dec_state_input_h],\n",
        "    [dec_outputs2] + [state_h2]\n",
        ")\n",
        "\n",
        "# Dictionary for reverse word mapping\n",
        "reverse_tar_word_index = tar_tokenizer.index_word\n",
        "reverse_inp_word_index = inp_tokenizer.index_word\n",
        "tar_word_index = tar_tokenizer.word_index\n",
        "reverse_tar_word_index[0] = ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ff9sEfhZai-m"
      },
      "outputs": [],
      "source": [
        "# Function to decode the sequence\n",
        "def decode_sequence(inp_seq):\n",
        "    # Get encoder outputs and hidden state by passing the input sequence\n",
        "    enc_out, enc_h = enc_model.predict(inp_seq)\n",
        "\n",
        "    # Target sequence starts with the initial word 'sos'\n",
        "    tar_seq = np.zeros((1, 1))\n",
        "    tar_seq[0, 0] = tar_word_index['sos']\n",
        "\n",
        "    # Stop condition and decoded sentence\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Get predicted output words and the updated hidden state\n",
        "        output_words, dec_h = dec_model.predict([tar_seq] + [enc_out, enc_h])\n",
        "\n",
        "        # Using the predicted index, find the corresponding word\n",
        "        word_index = np.argmax(output_words[0, -1, :])\n",
        "        text_word = reverse_tar_word_index[word_index]\n",
        "        decoded_sentence += text_word + \" \"\n",
        "\n",
        "        # Stop when we hit the terminal word 'eos' or reach max length\n",
        "        if text_word == \"eos\" or len(decoded_sentence) > max_tar_len:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update target sequence and hidden state for the next prediction\n",
        "        tar_seq = np.zeros((1, 1))\n",
        "        tar_seq[0, 0] = word_index\n",
        "        enc_h = dec_h\n",
        "\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IP6T8RGalRD",
        "outputId": "681f032e-7105-4e43-9670-8393ba4ef754"
      },
      "outputs": [],
      "source": [
        "#dict with key as label and value as target label value\n",
        "lab_val=dict((i,v) for i,v in zip(tar_lab,tar_lab_val))\n",
        "\n",
        "# Test the model with test data\n",
        "for i in range(0, 20, 3):\n",
        "    # Tokenize the test input and convert it into integer sequences\n",
        "    inp_x = inp_tokenizer.texts_to_sequences([x_test[i]])\n",
        "    # Pad the input to match the required input length\n",
        "    inp_x = pad_sequences(inp_x, maxlen=max_inp_len, padding='post')\n",
        "    # Reshape and decode the input sequence\n",
        "    tag = decode_sequence(inp_x.reshape(1, max_inp_len)).replace('eos', '')\n",
        "    print(\"Tweet:\", x_test[i])\n",
        "    print(\"Predicted Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in word_tokenize(tag)]))\n",
        "    print(\"Actual Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in y_test[i][4:-4].split(\" \")]))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rugT9i767jI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
