{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8MrX9IgHGJ_",
        "outputId": "e948be48-afde-4a41-ddb2-0a2418c93225"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from statistics import mode\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import RNN,Input, LSTM, Bidirectional, Embedding,Dense,Concatenate,Attention, Average\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkc8TbNGHt7F",
        "outputId": "6989e4bb-90ca-4032-fd3e-45346e14156a"
      },
      "outputs": [],
      "source": [
        "#read the dataset file\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "#tweet column is input\n",
        "inp_data=train[\"tweet\"]\n",
        "#target data is sentiment(s1,s2,s3,s4,s5) ,\n",
        "#when (w1,w2,w3,w4) and kind(k1,k2,k3...k15)\n",
        "tar_data=train.iloc[:,4:].values\n",
        "print(\"tar data:\", tar_data)\n",
        "\n",
        "#get the column name of target\n",
        "tar_lab=train.iloc[:,4:].columns.tolist()\n",
        "print(\"tar_lab:\", tar_lab)\n",
        "#value of the target label like\n",
        "#s1=\"I can't tell\" , s2=\"Negative\" and so on till s5\n",
        "#w1=\"current weather\", w2=future forecast and so on till w4\n",
        "#k1=\"clouds\", k2=\"cold\", k3=\"dry\" and so on till k15\n",
        "tar_lab_val=[\n",
        "\"I can't tell\",\"Negative\",\"Neutral\",\"Positive\",\"Tweet not related to weather condition\",\n",
        "\"current (same day) weather\",\"future (forecast)\",\"I can't tell\",\"past weather\",\n",
        "\"clouds\",\"cold\",\"dry\",\"hot\",\"humid\",\"hurricane\",\"I can't tell\",\"ice\",\"other\",\"rain\",\n",
        "\"snow\",\"storms\",\"sun\",\"tornado\",\"wind\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrcOwyZQIb4_"
      },
      "outputs": [],
      "source": [
        "#clean the tweets\n",
        "def clean(tweet):\n",
        "  #replace and lower the tweets\n",
        "  tweet=tweet.replace(\":\",\"\").lower()\n",
        "  #get only words that contains alphabets\n",
        "  words= list(filter(lambda w:(w.isalpha()),tweet.split(\" \")))\n",
        "  #expand the shortened words\n",
        "  words= [contractions[w] if w in contractions else w for w in words ]\n",
        "  #return all the words\n",
        "  return words\n",
        "\n",
        "\n",
        "inp_texts=[]\n",
        "tar_texts=[]\n",
        "inp_words=[]\n",
        "tar_words=[]\n",
        "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAKObdl3I9lD",
        "outputId": "024a4ee0-5d42-440e-ed4a-cf4ec413e909"
      },
      "outputs": [],
      "source": [
        "#iterate over input data\n",
        "for tweet in inp_data:\n",
        "  #clean the tweets\n",
        "  inpt_words= clean(tweet)\n",
        "  #store the input texts and words\n",
        "  inp_texts+= [' '.join(inpt_words)]\n",
        "  inp_words+= inpt_words\n",
        "\n",
        "#iterate over target data\n",
        "for lab in tar_data:\n",
        "  #get index of maximum value from sentiment data(s1 to s5)\n",
        "  #with the help of this index get label value\n",
        "  senti=tar_lab[np.argmax(lab[:5])]\n",
        "  #get index of maximum value from when data(w1 to w4)\n",
        "  #with the help of this index get label value\n",
        "  when=tar_lab[np.argmax(lab[5:9])+5]\n",
        "  #get index of values greater than 0.5 and get label value from it\n",
        "  kind=[tar_lab[ind] for ind,ele in enumerate(lab[9:len(lab)],9) if ele>=0.5]\n",
        "  #store the target text which is combination of sentiment,when and kind data\n",
        "  #add sos at start and eos at end of text\n",
        "  tar_texts+=[\"sos \"+\" \".join([senti]+[when]+kind)+\" eos\"]\n",
        "\n",
        "print(\"inp_texts:\", inp_texts[1:5])\n",
        "print(\"tar_texts:\", tar_texts[1:5])\n",
        "print(\"inp_words:\", inp_words[1:5])\n",
        "print(\"tar_words:\", tar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQhGwqpUIy6V",
        "outputId": "99ae3a28-846e-4cb5-c2d3-7d38892fb3dd"
      },
      "outputs": [],
      "source": [
        "#only store unique words from the input and target word lists\n",
        "inp_words = sorted(list(set(inp_words)))\n",
        "num_inp_words = len(inp_words)\n",
        "num_tar_words = len(tar_lab)+2\n",
        "\n",
        "#get the length of the input and the target texts which appears most frequently\n",
        "max_inp_len = mode([len(i) for i in inp_texts])\n",
        "max_tar_len = mode([len(i) for i in tar_texts])\n",
        "\n",
        "print(\"number of input words : \",num_inp_words)\n",
        "print(\"number of target words : \",num_tar_words)\n",
        "print(\"maximum input length : \",max_inp_len)\n",
        "print(\"maximum target length : \",max_tar_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rz5dNpKMYtv",
        "outputId": "8cf807cb-8a15-4366-d292-542b0747a2f9"
      },
      "outputs": [],
      "source": [
        "#split the input and target text into 90:10 ratio or testing size of 10%=0.1.\n",
        "x_train,x_test,y_train,y_test=train_test_split(inp_texts,tar_texts,test_size=0.1,random_state=42)\n",
        "\n",
        "#Use all of the words from training input and output to train the tokenizer.\n",
        "inp_tokenizer = Tokenizer()\n",
        "inp_tokenizer.fit_on_texts(x_train)\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#convert text to an integer sequence where the integer represents the word index\n",
        "x_train= inp_tokenizer.texts_to_sequences(x_train)\n",
        "y_train= tar_tokenizer.texts_to_sequences(y_train)\n",
        "\n",
        "print(\"x_train:\",x_train[1:5])\n",
        "print(\"y_train:\",y_train[1:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUJfGPCOsTOj"
      },
      "outputs": [],
      "source": [
        "#If the length is less than the maximum length, pad the array with 0s.\n",
        "enc_inp_data= pad_sequences(x_train, maxlen=max_inp_len, padding='post',dtype=\"float32\")\n",
        "dec_data= pad_sequences(y_train, maxlen=max_tar_len, padding='post',dtype=\"float32\")\n",
        "\n",
        "#The last word, ie 'eos,' will not be included in the decoder input data.\n",
        "dec_inp_data = dec_data[:,:-1]\n",
        "\n",
        "#decoder target data will be one time step ahead as it will not include the first initial word i.e 'sos'\n",
        "dec_tar_data = dec_data.reshape(len(dec_data),max_tar_len,1)[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PLTV9ioktg7-",
        "outputId": "47f5c649-d433-496f-a2c4-6c9a1df600a7"
      },
      "outputs": [],
      "source": [
        "# Model formation\n",
        "\n",
        "K.clear_session()\n",
        "latent_dim = 500\n",
        "\n",
        "#create input object with the shape equal to the maximum number of input words\n",
        "enc_inputs = Input(shape=(max_inp_len,))\n",
        "enc_embedding = Embedding(num_inp_words+1, latent_dim)(enc_inputs)\n",
        "\n",
        "#create 3 stacked LSTM layer\n",
        "#1st LSTM layer keep only output\n",
        "enc_lstm1= Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True), merge_mode=\"concat\", weights=None, backward_layer=None)\n",
        "enc_outputs1, *_ = enc_lstm1(enc_embedding)\n",
        "\n",
        "#2nd LSTM layer keep only output\n",
        "enc_lstm2= Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True), merge_mode=\"concat\", weights=None, backward_layer=None)\n",
        "enc_outputs2, *_ = enc_lstm2(enc_outputs1)\n",
        "\n",
        "#3rd LSTM layer keep output as well as its states\n",
        "enc_lstm3= Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True), merge_mode=\"concat\", weights=None, backward_layer=None)\n",
        "# enc_outputs3 , state_h3 , state_c3= enc_lstm3(enc_outputs2)\n",
        "\n",
        "# Get the outputs and states from the Bidirectional LSTM\n",
        "enc_outputs3, forward_h, forward_c, backward_h, backward_c = enc_lstm3(enc_outputs2)\n",
        "\n",
        "# Combine forward and backward states\n",
        "# state_h3 = Average()([forward_h, backward_h])  # Average forward and backward hidden states\n",
        "# state_c3 = Average()([forward_c, backward_c])  # Average forward and backward cell states\n",
        "\n",
        "# #encoder states\n",
        "# enc_states= [state_h3, state_c3]\n",
        "\n",
        "# Decoder.\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_tar_words+1, latent_dim)\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "#initialize the LSTM layer of the decoder with the encoder's output states\n",
        "dec_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True), merge_mode=\"concat\", weights=None, backward_layer=None)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding, initial_state=[forward_h, forward_c, backward_h, backward_c])\n",
        "\n",
        "#Attention layer\n",
        "attention =Attention()\n",
        "attn_out = attention([dec_outputs,enc_outputs3])\n",
        "\n",
        "#Merge the attention output with the decoder outputs\n",
        "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])\n",
        "\n",
        "#fully connected Dense layer for the output\n",
        "dec_dense = Dense(num_tar_words+1, activation='softmax')\n",
        "dec_outputs = dec_dense(merge)\n",
        "\n",
        "#Model class and model summary\n",
        "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "model.summary()\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt0nkBDczKGn"
      },
      "outputs": [],
      "source": [
        "#compile the model using RMSProp optimizer\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#train the model with input and target data from encoder and decoder\n",
        "# model.fit( [enc_inp_data, dec_inp_data], dec_tar_data, batch_size=500, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVjFpbdCAJDT"
      },
      "outputs": [],
      "source": [
        "#Save model with the name as “s2s”\n",
        "# model.save('my_model.keras')\n",
        "\n",
        "#encoder inference\n",
        "latent_dim=500\n",
        "\n",
        "# #load the model\n",
        "model = models.load_model(\"BiLSTM.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIkEAQkKHeVD"
      },
      "outputs": [],
      "source": [
        "# Construct an encoder model from the output of the BiLSTM layer (assume 6th layer is BiLSTM)\n",
        "enc_outputs, state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd = model.layers[6].output\n",
        "\n",
        "# Combine forward and backward states for the BiLSTM encoder\n",
        "enc_states = [state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd]\n",
        "\n",
        "# Add input data and state data from the layer\n",
        "enc_model = Model(model.input[0], [enc_outputs] + enc_states)\n",
        "\n",
        "# Decoder inference model\n",
        "# Create Input objects for hidden and cell states for decoder (forward and backward states)\n",
        "dec_state_input_h_fwd = Input(shape=(latent_dim,))\n",
        "dec_state_input_c_fwd = Input(shape=(latent_dim,))\n",
        "dec_state_input_h_bwd = Input(shape=(latent_dim,))\n",
        "dec_state_input_c_bwd = Input(shape=(latent_dim,))\n",
        "\n",
        "# Input for encoder outputs\n",
        "dec_hidden_state_input = Input(shape=(max_inp_len, latent_dim * 2))  # *2 for BiLSTM\n",
        "\n",
        "# Get all the layers from the model\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_lstm = model.layers[7]\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "# Add input and initialize the BiLSTM layer with decoder’s hidden and cell states\n",
        "dec_outputs2, state_h_fwd2, state_c_fwd2, state_h_bwd2, state_c_bwd2 = dec_lstm(\n",
        "    dec_embedding,\n",
        "    initial_state=[dec_state_input_h_fwd, dec_state_input_c_fwd, dec_state_input_h_bwd, dec_state_input_c_bwd]\n",
        ")\n",
        "\n",
        "# Attention layer\n",
        "attention = model.layers[8]\n",
        "attn_out1 = attention([dec_outputs2, dec_hidden_state_input])\n",
        "\n",
        "# Merge attention output with decoder outputs\n",
        "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out1])\n",
        "\n",
        "# Dense layer for decoder output\n",
        "dec_dense = model.layers[10]\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# Define the Decoder model Class\n",
        "dec_model = Model(\n",
        "    [dec_inputs]\n",
        "    + [dec_hidden_state_input, dec_state_input_h_fwd, dec_state_input_c_fwd, dec_state_input_h_bwd, dec_state_input_c_bwd],\n",
        "    [dec_outputs2, state_h_fwd2, state_c_fwd2, state_h_bwd2, state_c_bwd2],\n",
        ")\n",
        "\n",
        "# Create a dictionary with all indexes as keys and respective target labels as values\n",
        "reverse_tar_word_index = tar_tokenizer.index_word\n",
        "reverse_inp_word_index = inp_tokenizer.index_word\n",
        "tar_word_index = tar_tokenizer.word_index\n",
        "reverse_tar_word_index[0] = \" \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLYx-7gwINX6"
      },
      "outputs": [],
      "source": [
        "# #Attention layer\n",
        "# attention = model.layers[8]\n",
        "# attn_out1 = attention([dec_outputs2,dec_hidden_state_input])\n",
        "\n",
        "# merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out1])\n",
        "\n",
        "# #Dense layer for decoder output\n",
        "# dec_dense = model.layers[10]\n",
        "# dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# # Finally define the Decoder model Class\n",
        "# dec_model = Model(\n",
        "# [dec_inputs] + [dec_hidden_state_input,dec_state_input_h, dec_state_input_c],\n",
        "# [dec_outputs2] + [state_h2, state_h3, state_c2, state_c3])\n",
        "\n",
        "# #create a dictionary with all indexes as key and respective target label as values\n",
        "# reverse_tar_word_index = tar_tokenizer.index_word\n",
        "# reverse_inp_word_index = inp_tokenizer.index_word\n",
        "# tar_word_index = tar_tokenizer.word_index\n",
        "# reverse_tar_word_index[0]=' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjO10uNFISYS"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(inp_seq):\n",
        "    # Get the encoder outputs and forward/backward states (hidden and cell) by passing the input sequence\n",
        "    enc_out, enc_h_fwd, enc_c_fwd, enc_h_bwd, enc_c_bwd = enc_model.predict(inp_seq, verbose=0)\n",
        "\n",
        "    # Target sequence with the initial word as 'sos'\n",
        "    tar_seq = np.zeros((1, 1))\n",
        "    tar_seq[0, 0] = tar_word_index['sos']\n",
        "\n",
        "    # Stop the iteration if the iteration reaches the end of the text\n",
        "    stop_condition = False\n",
        "    # Merge every predicted word in the decoded sentence\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        # Get predicted output words, forward/backward hidden, and cell states for the model\n",
        "        output_words, dec_h_fwd, dec_c_fwd, dec_h_bwd, dec_c_bwd = dec_model.predict(\n",
        "            [tar_seq] + [enc_out, enc_h_fwd, enc_c_fwd, enc_h_bwd, enc_c_bwd], verbose=0\n",
        "        )\n",
        "\n",
        "        # Using the index, get the word from the dictionary\n",
        "        word_index = np.argmax(output_words[0, -1, :])\n",
        "        text_word = reverse_tar_word_index[word_index]\n",
        "        decoded_sentence += text_word + \" \"\n",
        "\n",
        "        # Stop when we either hit the max length or reach the terminal word \"eos\".\n",
        "        if text_word == \"eos\" or len(decoded_sentence) > max_tar_len:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence with the current word index\n",
        "        tar_seq = np.zeros((1, 1))\n",
        "        tar_seq[0, 0] = word_index\n",
        "\n",
        "        # Update the forward and backward hidden and cell states\n",
        "        enc_h_fwd, enc_c_fwd = dec_h_fwd, dec_c_fwd\n",
        "        enc_h_bwd, enc_c_bwd = dec_h_bwd, dec_c_bwd\n",
        "\n",
        "    # Return the decoded sentence string\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyosNEndS0u0",
        "outputId": "8580c636-789b-444c-f72b-f08192cf3d0a"
      },
      "outputs": [],
      "source": [
        "#dict with key as label and value as target label value\n",
        "lab_val=dict((i,v) for i,v in zip(tar_lab,tar_lab_val))\n",
        "\n",
        "correct_predictions = 0\n",
        "total_hashtag_comparisons = 0\n",
        "matching_hashtags = 0\n",
        "total_predictions = len(x_test)\n",
        "\n",
        "print(\"length:\", total_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H7f3GAbIYQO",
        "outputId": "4eaf3773-da59-443d-f16d-0ad0393baced"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Test the model with test data\n",
        "# for i in range(0, total_predictions, 1):\n",
        "#     if(i%50==0):\n",
        "#       print(\"i:\", i)\n",
        "# Tokenize the test input and convert it into integer sequences\n",
        "inp_x = inp_tokenizer.texts_to_sequences([x_test[0]])\n",
        "# Pad the input to match the required input length\n",
        "inp_x = pad_sequences(inp_x, maxlen=max_inp_len, padding='post')\n",
        "# Reshape and decode the input sequence\n",
        "tag = decode_sequence(inp_x.reshape(1, max_inp_len)).replace('eos', '')\n",
        "predicted_hashtags = set([\"#\" + lab_val[j] for j in word_tokenize(tag)])\n",
        "actual_hashtags = set([\"#\" + lab_val[j] for j in y_test[0][4:-4].split(\" \")])\n",
        "# print(\"Tweet:\", x_test[i])\n",
        "# print(\"Predicted Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in word_tokenize(tag)]))\n",
        "# print(\"Actual Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in y_test[i][4:-4].split(\" \")]))\n",
        "# print(\"\\n\")\n",
        "# Increment the total hashtag comparisons\n",
        "# total_hashtag_comparisons += len(actual_hashtags\n",
        "# Count the number of correctly predicted hashtags\n",
        "# matching_hashtags += len(predicted_hashtags.intersection(actual_hashtags))\n",
        "\n",
        "# Compute the accuracy score\n",
        "# accuracy_score = (matching_hashtags / total_hashtag_comparisons) * 100\n",
        "# print(f\"Hashtag-Level Accuracy Score: {accuracy_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8537z4m1IzRq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
