{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8MrX9IgHGJ_",
        "outputId": "3c20bab5-7f9d-4a00-e16e-487908355f25"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from statistics import mode\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import RNN,Input,LSTM, Bidirectional ,Embedding,Dense,Concatenate,Attention\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkc8TbNGHt7F",
        "outputId": "7f413b73-525f-4b5a-9930-ba36ff81c62f"
      },
      "outputs": [],
      "source": [
        "#read the dataset file\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "#tweet column is input\n",
        "inp_data=train[\"tweet\"]\n",
        "#target data is sentiment(s1,s2,s3,s4,s5) ,\n",
        "#when (w1,w2,w3,w4) and kind(k1,k2,k3...k15)\n",
        "tar_data=train.iloc[:,4:].values\n",
        "print(\"tar data:\", tar_data)\n",
        "\n",
        "#get the column name of target\n",
        "tar_lab=train.iloc[:,4:].columns.tolist()\n",
        "print(\"tar_lab:\", tar_lab)\n",
        "#value of the target label like\n",
        "#s1=\"I can't tell\" , s2=\"Negative\" and so on till s5\n",
        "#w1=\"current weather\", w2=future forecast and so on till w4\n",
        "#k1=\"clouds\", k2=\"cold\", k3=\"dry\" and so on till k15\n",
        "tar_lab_val=[\n",
        "\"I can't tell\",\"Negative\",\"Neutral\",\"Positive\",\"Tweet not related to weather condition\",\n",
        "\"current (same day) weather\",\"future (forecast)\",\"I can't tell\",\"past weather\",\n",
        "\"clouds\",\"cold\",\"dry\",\"hot\",\"humid\",\"hurricane\",\"I can't tell\",\"ice\",\"other\",\"rain\",\n",
        "\"snow\",\"storms\",\"sun\",\"tornado\",\"wind\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RrcOwyZQIb4_"
      },
      "outputs": [],
      "source": [
        "#clean the tweets\n",
        "def clean(tweet):\n",
        "  #replace and lower the tweets\n",
        "  tweet=tweet.replace(\":\",\"\").lower()\n",
        "  #get only words that contains alphabets\n",
        "  words= list(filter(lambda w:(w.isalpha()),tweet.split(\" \")))\n",
        "  #expand the shortened words\n",
        "  words= [contractions[w] if w in contractions else w for w in words ]\n",
        "  #return all the words\n",
        "  return words\n",
        "\n",
        "\n",
        "inp_texts=[]\n",
        "tar_texts=[]\n",
        "inp_words=[]\n",
        "tar_words=[]\n",
        "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAKObdl3I9lD",
        "outputId": "b62ed028-8c98-451a-e3a5-90964d1c8a93"
      },
      "outputs": [],
      "source": [
        "#iterate over input data\n",
        "for tweet in inp_data:\n",
        "  #clean the tweets\n",
        "  inpt_words= clean(tweet)\n",
        "  #store the input texts and words\n",
        "  inp_texts+= [' '.join(inpt_words)]\n",
        "  inp_words+= inpt_words\n",
        "\n",
        "#iterate over target data\n",
        "for lab in tar_data:\n",
        "  #get index of maximum value from sentiment data(s1 to s5)\n",
        "  #with the help of this index get label value\n",
        "  senti=tar_lab[np.argmax(lab[:5])]\n",
        "  #get index of maximum value from when data(w1 to w4)\n",
        "  #with the help of this index get label value\n",
        "  when=tar_lab[np.argmax(lab[5:9])+5]\n",
        "  #get index of values greater than 0.5 and get label value from it\n",
        "  kind=[tar_lab[ind] for ind,ele in enumerate(lab[9:len(lab)],9) if ele>=0.5]\n",
        "  #store the target text which is combination of sentiment,when and kind data\n",
        "  #add sos at start and eos at end of text\n",
        "  tar_texts+=[\"sos \"+\" \".join([senti]+[when]+kind)+\" eos\"]\n",
        "\n",
        "print(\"inp_texts:\", inp_texts[1:5])\n",
        "print(\"tar_texts:\", tar_texts[1:5])\n",
        "print(\"inp_words:\", inp_words[1:5])\n",
        "print(\"tar_words:\", tar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQhGwqpUIy6V",
        "outputId": "d5615517-7d8f-4ad6-c19c-0f756b33b25b"
      },
      "outputs": [],
      "source": [
        "#only store unique words from the input and target word lists\n",
        "inp_words = sorted(list(set(inp_words)))\n",
        "num_inp_words = len(inp_words)\n",
        "num_tar_words = len(tar_lab)+2\n",
        "\n",
        "#get the length of the input and the target texts which appears most frequently\n",
        "max_inp_len = mode([len(i) for i in inp_texts])\n",
        "max_tar_len = mode([len(i) for i in tar_texts])\n",
        "\n",
        "print(\"number of input words : \",num_inp_words)\n",
        "print(\"number of target words : \",num_tar_words)\n",
        "print(\"maximum input length : \",max_inp_len)\n",
        "print(\"maximum target length : \",max_tar_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rz5dNpKMYtv",
        "outputId": "4d2c3a94-a61f-4c7e-d58c-7215f68a767c"
      },
      "outputs": [],
      "source": [
        "#split the input and target text into 90:10 ratio or testing size of 10%=0.1.\n",
        "x_train,x_test,y_train,y_test=train_test_split(inp_texts,tar_texts,test_size=0.1,random_state=42)\n",
        "\n",
        "#Use all of the words from training input and output to train the tokenizer.\n",
        "inp_tokenizer = Tokenizer()\n",
        "inp_tokenizer.fit_on_texts(x_train)\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#convert text to an integer sequence where the integer represents the word index\n",
        "x_train= inp_tokenizer.texts_to_sequences(x_train)\n",
        "y_train= tar_tokenizer.texts_to_sequences(y_train)\n",
        "\n",
        "print(\"x_train:\",x_train[1:5])\n",
        "print(\"y_train:\",y_train[1:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kUJfGPCOsTOj"
      },
      "outputs": [],
      "source": [
        "#If the length is less than the maximum length, pad the array with 0s.\n",
        "enc_inp_data= pad_sequences(x_train, maxlen=max_inp_len, padding='post',dtype=\"float32\")\n",
        "dec_data= pad_sequences(y_train, maxlen=max_tar_len, padding='post',dtype=\"float32\")\n",
        "\n",
        "#The last word, ie 'eos,' will not be included in the decoder input data.\n",
        "dec_inp_data = dec_data[:,:-1]\n",
        "\n",
        "#decoder target data will be one time step ahead as it will not include the first initial word i.e 'sos'\n",
        "dec_tar_data = dec_data.reshape(len(dec_data),max_tar_len,1)[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PLTV9ioktg7-",
        "outputId": "7762ac9a-3fde-48ad-a016-b3f94867a116"
      },
      "outputs": [],
      "source": [
        "# Model formation\n",
        "\n",
        "K.clear_session()\n",
        "latent_dim = 500\n",
        "\n",
        "#create input object with the shape equal to the maximum number of input words\n",
        "enc_inputs = Input(shape=(max_inp_len,))\n",
        "enc_embedding = Embedding(num_inp_words+1, latent_dim)(enc_inputs)\n",
        "\n",
        "#create 3 stacked LSTM layer\n",
        "#1st LSTM layer keep only output\n",
        "enc_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "enc_outputs1, *_ = enc_lstm1(enc_embedding)\n",
        "\n",
        "#2nd LSTM layer keep only output\n",
        "enc_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "enc_outputs2, *_ = enc_lstm2(enc_outputs1)\n",
        "\n",
        "#3rd LSTM layer keep output as well as its states\n",
        "enc_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "enc_outputs3 , state_h3 , state_c3= enc_lstm3(enc_outputs2)\n",
        "\n",
        "#encoder states\n",
        "enc_states= [state_h3, state_c3]\n",
        "\n",
        "# Decoder.\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_tar_words+1, latent_dim)\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "#initialize the LSTM layer of the decoder with the encoder's output states\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=enc_states)\n",
        "\n",
        "#Attention layer\n",
        "attention =Attention()\n",
        "attn_out = attention([dec_outputs,enc_outputs3])\n",
        "\n",
        "#Merge the attention output with the decoder outputs\n",
        "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])\n",
        "\n",
        "#fully connected Dense layer for the output\n",
        "dec_dense = Dense(num_tar_words+1, activation='softmax')\n",
        "dec_outputs = dec_dense(merge)\n",
        "\n",
        "#Model class and model summary\n",
        "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "model.summary()\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt0nkBDczKGn",
        "outputId": "2a811303-5dfb-47a2-de39-8808d44ea6de"
      },
      "outputs": [],
      "source": [
        "#compile the model using RMSProp optimizer\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#train the model with input and target data from encoder and decoder\n",
        "model.fit(\n",
        "    [enc_inp_data, dec_inp_data],dec_tar_data,\n",
        "    batch_size=500,epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mVjFpbdCAJDT"
      },
      "outputs": [],
      "source": [
        "#Save model with the name as “s2s”\n",
        "model.save('LSTM.keras')\n",
        "\n",
        "#encoder inference\n",
        "latent_dim=500\n",
        "\n",
        "# #load the model\n",
        "# model = models.load_model(\"my_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lIkEAQkKHeVD"
      },
      "outputs": [],
      "source": [
        "#construct an encoder model from the output of the 6th layer of LSTM\n",
        "enc_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
        "enc_states=[state_h_enc,state_c_enc]\n",
        "\n",
        "#add input data and state data from the layer\n",
        "enc_model = Model(model.input[0],[enc_outputs]+enc_states)\n",
        "\n",
        "# decoder inference model\n",
        "#create Input object of hidden state and cell state for decoder\n",
        "dec_state_input_h = Input(shape=(latent_dim,))\n",
        "dec_state_input_c = Input(shape=(latent_dim,))\n",
        "dec_hidden_state_input = Input(shape=(max_inp_len,latent_dim))\n",
        "\n",
        "#Get the all the layers from the model\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_lstm = model.layers[7]\n",
        "dec_embedding= dec_emb_layer(dec_inputs)\n",
        "\n",
        "#add input and initialize the LSTM layer with decoder’s hidden and cell state\n",
        "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zLYx-7gwINX6"
      },
      "outputs": [],
      "source": [
        "#Attention layer\n",
        "attention = model.layers[8]\n",
        "attn_out1 = attention([dec_outputs2,dec_hidden_state_input])\n",
        "\n",
        "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out1])\n",
        "\n",
        "#Dense layer for decoder output\n",
        "dec_dense = model.layers[10]\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# Finally define the Decoder model Class\n",
        "dec_model = Model(\n",
        "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
        "[dec_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "#create a dictionary with all indexes as key and respective target label as values\n",
        "reverse_tar_word_index = tar_tokenizer.index_word\n",
        "reverse_inp_word_index = inp_tokenizer.index_word\n",
        "tar_word_index = tar_tokenizer.word_index\n",
        "reverse_tar_word_index[0]=' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YjO10uNFISYS"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(inp_seq):\n",
        "    #get the encoder outputs and states(hidden and cell) by passing the input sequence\n",
        "    enc_out, enc_h, enc_c= enc_model.predict(inp_seq, verbose=0)\n",
        "\n",
        "    #target sequence with starting initial word as 'sos'\n",
        "    tar_seq = np.zeros((1, 1))\n",
        "    tar_seq[0, 0] = tar_word_index['sos']\n",
        "\n",
        "    #Stop the iteration if the iteration reaches end of the text\n",
        "    stop_condition = False\n",
        "    #merge every predicted word in decoded sentence\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "      #get predicted output words, hidden and cell state for the model\n",
        "      output_words, dec_h, dec_c= dec_model.predict([tar_seq] + [enc_out,enc_h, enc_c], verbose=0)\n",
        "\n",
        "      #Using index get the word from the dictionary\n",
        "      word_index = np.argmax(output_words[0, -1, :])\n",
        "      text_word = reverse_tar_word_index[word_index]\n",
        "      decoded_sentence += text_word +\" \"\n",
        "\n",
        "      # Stop when we either hit max length or reach the terminal word i.e. eos.\n",
        "      if text_word == \"eos\" or len(decoded_sentence) > max_tar_len:\n",
        "          stop_condition = True\n",
        "\n",
        "      #update target sequence with the current word index.\n",
        "      tar_seq = np.zeros((1, 1))\n",
        "      tar_seq[0, 0] = word_index\n",
        "      enc_h, enc_c = dec_h, dec_c\n",
        "\n",
        "\n",
        "    #return the decoded sentence string\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXugbfsKmlkG",
        "outputId": "08df519b-7ca7-4c98-b3c7-5c1b141dbe37"
      },
      "outputs": [],
      "source": [
        "#dict with key as label and value as target label value\n",
        "lab_val=dict((i,v) for i,v in zip(tar_lab,tar_lab_val))\n",
        "\n",
        "correct_predictions = 0\n",
        "total_hashtag_comparisons = 0\n",
        "matching_hashtags = 0\n",
        "total_predictions = len(x_test)\n",
        "\n",
        "print(\"length:\", total_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw5NoXV6mruz",
        "outputId": "37d9b1f3-bed8-4df6-e88f-2c1cec768155"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Test the model with test data\n",
        "for i in range(0, total_predictions, 1):\n",
        "    if(i%50==0):\n",
        "      print(\"i:\", i)\n",
        "    # Tokenize the test input and convert it into integer sequences\n",
        "    inp_x = inp_tokenizer.texts_to_sequences([x_test[0]])\n",
        "    # Pad the input to match the required input length\n",
        "    inp_x = pad_sequences(inp_x, maxlen=max_inp_len, padding='post')\n",
        "    # Reshape and decode the input sequence\n",
        "    tag = decode_sequence(inp_x.reshape(1, max_inp_len)).replace('eos', '')\n",
        "    predicted_hashtags = set([\"#\" + lab_val[j] for j in word_tokenize(tag)])\n",
        "    actual_hashtags = set([\"#\" + lab_val[j] for j in y_test[0][4:-4].split(\" \")])\n",
        "    # print(\"Tweet:\", x_test[i])\n",
        "    # print(\"Predicted Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in word_tokenize(tag)]))\n",
        "    # print(\"Actual Hashtag:\", \" \".join([\"#\" + lab_val[i] for i in y_test[i][4:-4].split(\" \")]))\n",
        "    # print(\"\\n\")\n",
        "    # Increment the total hashtag comparisons\n",
        "    total_hashtag_comparisons += len(actual_hashtags)\n",
        "    # Count the number of correctly predicted hashtags\n",
        "    matching_hashtags += len(predicted_hashtags.intersection(actual_hashtags))\n",
        "\n",
        "# Compute the accuracy score\n",
        "accuracy_score = (matching_hashtags / total_hashtag_comparisons) * 100\n",
        "print(f\"Hashtag-Level Accuracy Score: {accuracy_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H7f3GAbIYQO",
        "outputId": "a71cd7e4-e348-4b3f-abef-fc2295d415e0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8537z4m1IzRq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
